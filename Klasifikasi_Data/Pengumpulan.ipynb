{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQvGxveQX-x6",
        "outputId": "518e85eb-57bf-4506-f503-609c774d1be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ek7YpZDzVzzj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Library Machine Learning (Scikit-Learn)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Library Deep Learning (TensorFlow/Keras)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Input, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKGmP8bGWB0v",
        "outputId": "3b2d8831-7fef-4567-b6ea-b52f0ca88b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Data: 315\n",
            "                   tokoh                                               text  \\\n",
            "0   Purbaya Yudhi Sadewa  sehat2 bapak presiden ku dan pak menkeu selalu...   \n",
            "1  Sri Mulyani Indrawati  Di luar negri lumrah pejabat mundur, di NKRI i...   \n",
            "2  Sri Mulyani Indrawati  Pajak rakyat terus ditekan, pajak pengusaha ti...   \n",
            "3   Purbaya Yudhi Sadewa  Pihak2 yang selama ini merasa tdk diawasi dlm ...   \n",
            "4   Purbaya Yudhi Sadewa                                 Pemimpin itu laki2   \n",
            "\n",
            "     label  \n",
            "0  positif  \n",
            "1  negatif  \n",
            "2  negatif  \n",
            "3   netral  \n",
            "4   netral  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/dataset_labelling_manual.csv')\n",
        "print(f\"Total Data: {len(df)}\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd0F8nNYWKXs",
        "outputId": "f01c77c7-cb04-46b3-8d1f-6fdc25e7e08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menginisialisasi Sastrawi Stemmer...\n"
          ]
        }
      ],
      "source": [
        "# --- KONFIGURASI UMUM ---\n",
        "MAX_FEATURES = 5000\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "FILE_MANUAL = 'hasil_labelling_manual.csv'\n",
        "FILE_FULL = 'dataset_tugas_purbaya_vs_srimulyani.csv'\n",
        "\n",
        "# --- INISIALISASI STEMMER (WAJIB ADA) ---\n",
        "# Diperlukan agar perintah stemmer.stem() di bawah berfungsi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "print(\"Menginisialisasi Sastrawi Stemmer...\")\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# --- KODE BARU: STOPWORDS & CLEANING ---\n",
        "# Import Stopword Factory dari Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# 1. SIAPKAN DAFTAR STOPWORDS\n",
        "factory_sw = StopWordRemoverFactory()\n",
        "stopwords_indo = factory_sw.get_stop_words()\n",
        "\n",
        "# Tambahan stopwords manual (kata yang sering muncul tapi tidak penting di WordCloud)\n",
        "more_stopwords = [\n",
        "    'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar',\n",
        "    'bikin', 'bilang', 'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si',\n",
        "    'tau', 'tdk', 'tuh', 'utk', 'ya', 'jd', 'sdh', 'aja', 'n',\n",
        "    't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt', 'bukan', 'ini',\n",
        "    'itu', 'ada', 'dan', 'dari', 'dia', 'ke', 'kita', 'mau', 'pada',\n",
        "    'saya', 'kami', 'anda', 'mereka', 'semua', 'sudah', 'tapi', 'atau',\n",
        "    'banyak', 'beberapa', 'biasa', 'bila', 'boleh', 'buat', 'bukan',\n",
        "    'cukup', 'cuma', 'dapat', 'dari', 'depan', 'diri', 'dulu', 'enggak',\n",
        "    'entah', 'hal', 'hampir', 'hanya', 'harus', 'hendak', 'ia', 'ingin',\n",
        "    'ini', 'itu', 'jadi', 'jika', 'juga', 'justru', 'kalau',\n",
        "    'kami', 'kamu', 'kan', 'kapan', 'karena', 'kata', 'ke', 'kembali',\n",
        "    'kenapa', 'kepada', 'ketika', 'kita', 'lagi', 'lalu', 'lain', 'lama',\n",
        "    'lewat', 'mana', 'mari', 'masih', 'mau', 'mengapa', 'mereka', 'maka',\n",
        "    'menurut', 'mungkin', 'nanti', 'namun', 'nyaris', 'oleh', 'pada',\n",
        "    'padahal', 'paling', 'para', 'pasti', 'per', 'pernah', 'pula', 'pun',\n",
        "    'saat', 'saja', 'sambil', 'sampai', 'sana', 'sangat', 'saya', 'sebab',\n",
        "    'bagai', 'seperti', 'sering', 'siapa', 'silakan', 'sini', 'suatu',\n",
        "    'sudah', 'supaya', 'tadi', 'tanpa', 'tapi', 'telah', 'tentang',\n",
        "    'tentu', 'tepat', 'terhadap', 'tetapi', 'tiap', 'toh', 'tunjuk',\n",
        "    'turut', 'untuk', 'waduh', 'wah', 'wahai', 'wong', 'yaitu', 'yakni', 'yang'\n",
        "]\n",
        "\n",
        "# 2. DAFTAR NAMA TOKOH (DIHAPUS AGAR TIDAK BIAS)\n",
        "stopwords_tokoh = [\n",
        "    'sri', 'mulyani', 'indrawati', 'bu sri', 'menkeu', 'menteri keuangan',\n",
        "    'purbaya', 'yudhi', 'sadewa', 'pak purbaya', 'purbaya sadewa',\n",
        "    'menteri', 'pak', 'bu', 'bapak', 'ibu', 'presiden', 'jokowi', 'prabowo'\n",
        "]\n",
        "\n",
        "# Gabungkan semua stopwords menjadi satu SET (agar pencarian cepat)\n",
        "# Set otomatis menghapus duplikat\n",
        "final_stopwords = set(stopwords_indo + more_stopwords + stopwords_tokoh)\n",
        "\n",
        "# 3. KAMUS SLANG\n",
        "slang_dict = {\n",
        "    'yg': 'yang', 'gk': 'tidak', 'ga': 'tidak', 'gak': 'tidak', 'enggak': 'tidak',\n",
        "    'bgt': 'banget', 'dlm': 'dalam', 'sy': 'saya', 'aku': 'saya', 'gw': 'saya',\n",
        "    'gue': 'saya', 'lu': 'kamu', 'lo': 'kamu', 'anda': 'kamu', 'kalo': 'kalau',\n",
        "    'kl': 'kalau', 'klo': 'kalau', 'dr': 'dari', 'dgn': 'dengan', 'krn': 'karena',\n",
        "    'jd': 'jadi', 'jdi': 'jadi', 'sdh': 'sudah', 'udh': 'sudah', 'blm': 'belum',\n",
        "    'tp': 'tapi', 'tpi': 'tapi', 'tak': 'tidak', 'tdk': 'tidak', 'jgn': 'jangan',\n",
        "    'utk': 'untuk', 'aja': 'saja', 'aj': 'saja', 'lbh': 'lebih', 'sbg': 'sebagai',\n",
        "    'bs': 'bisa', 'bisa': 'dapat', 'pd': 'pada', 'kmrn': 'kemarin', 'skrg': 'sekarang',\n",
        "    'trs': 'terus', 'bkn': 'bukan', 'ok': 'oke', 'thx': 'terima kasih',\n",
        "    'makasih': 'terima kasih', 'tks': 'terima kasih', 'gan': 'juragan',\n",
        "    'kak': 'kakak', 'min': 'admin', 'tau': 'tahu', 'mw': 'mau', 'mo': 'mau',\n",
        "    'lbh': 'lebih', 'memang': 'memang', 'emang': 'memang', 'kayak': 'seperti',\n",
        "    'kyk': 'seperti', 'kek': 'seperti', 'bpk': 'Bapak', 'tdk': 'tidak'\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Pipeline: Lowercase -> Regex -> Slang -> Stopwords Removal -> Stemming\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # A. Regex Cleaning\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    words = text.split()\n",
        "    final_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # B. Normalisasi Slang\n",
        "        word = slang_dict.get(word, word)\n",
        "\n",
        "        # C. STOPWORD REMOVAL\n",
        "        # Jika kata ada di daftar stopwords, lewati/hapus\n",
        "        if word in final_stopwords:\n",
        "            continue\n",
        "\n",
        "        # D. Hapus kata < 3 huruf (Double filter)\n",
        "        if len(word) < 3:\n",
        "            continue\n",
        "\n",
        "        final_words.append(word)\n",
        "\n",
        "    text = \" \".join(final_words)\n",
        "\n",
        "    # E. Stemming (Sastrawi)\n",
        "    if len(text) > 0:\n",
        "        text = stemmer.stem(text)\n",
        "\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXSXy0W6WXXd"
      },
      "source": [
        "PERCOBAAN 1: Bag of Words (BoW) + Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLA_mfYmWUig",
        "outputId": "677f4839-5163-4e4d-dd0c-f1c91f45d0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akurasi: 0.6190\n"
          ]
        }
      ],
      "source": [
        "vectorizer_bow = CountVectorizer(max_features=5000) # BoW Fitur\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train_text)\n",
        "X_test_bow = vectorizer_bow.transform(X_test_text)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_bow, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test_bow)\n",
        "\n",
        "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
        "results.append({\"Model\": \"Naive Bayes\", \"Feature\": \"BoW\", \"Accuracy\": acc_nb})\n",
        "print(f\"Akurasi: {acc_nb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVGSSe2RWkcM"
      },
      "source": [
        "PERCOBAAN 2: TF-IDF + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmggRvWyWhs4",
        "outputId": "88a89667-d3b4-41e8-dbc0-7458377a3363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akurasi: 0.6032\n"
          ]
        }
      ],
      "source": [
        "vectorizer_tfidf = TfidfVectorizer(max_features=5000) # TF-IDF Fitur\n",
        "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf = vectorizer_tfidf.transform(X_test_text)\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
        "\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "results.append({\"Model\": \"Logistic Regression\", \"Feature\": \"TF-IDF\", \"Accuracy\": acc_lr})\n",
        "print(f\"Akurasi: {acc_lr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwOzSNoTWoQo"
      },
      "source": [
        "PERCOBAAN 3: TF-IDF + SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZZnpo4YWqXA",
        "outputId": "235e5f01-6166-4498-e280-1845a54b8742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akurasi: 0.6032\n"
          ]
        }
      ],
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "results.append({\"Model\": \"SVM\", \"Feature\": \"TF-IDF\", \"Accuracy\": acc_svm})\n",
        "print(f\"Akurasi: {acc_svm:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiuVj4OWWthI"
      },
      "source": [
        "PERSIAPAN DEEP LEARNING (Word Embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzqFGa6zYdP3",
        "outputId": "0b594e1f-5aed-4b84-8b7a-9b52913f6700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sedang membuat kolom 'clean_text'...\n",
            "✓ Kolom 'clean_text' berhasil dibuat!\n",
            "                                                text  \\\n",
            "0  sehat2 bapak presiden ku dan pak menkeu selalu...   \n",
            "1  Di luar negri lumrah pejabat mundur, di NKRI i...   \n",
            "2  Pajak rakyat terus ditekan, pajak pengusaha ti...   \n",
            "3  Pihak2 yang selama ini merasa tdk diawasi dlm ...   \n",
            "4                                 Pemimpin itu laki2   \n",
            "\n",
            "                                          clean_text  \n",
            "0          sehat2 selalu untung lindung tuhan aamiin  \n",
            "1  luar negri lumrah jabat mundur nkri inginya pe...  \n",
            "2    pajak rakyat terus tekan pajak usaha transfaran  \n",
            "3  pihak2 lama rasa awas guna anggar sekarang mul...  \n",
            "4                                       pimpin laki2  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Sedang membuat kolom 'clean_text'...\")\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "df = df.dropna(subset=['clean_text'])\n",
        "df = df[df['clean_text'].str.strip().astype(bool)]\n",
        "\n",
        "print(\"✓ Kolom 'clean_text' berhasil dibuat!\")\n",
        "print(df[['text', 'clean_text']].head()) # Cek hasilnya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Uo-LvVGPW0D_"
      },
      "outputs": [],
      "source": [
        "# Tokenisasi untuk LSTM & Transformer\n",
        "MAX_WORDS = 5000\n",
        "MAX_LEN = 100\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "# Padding agar panjang kalimat sama\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zsAY7arW2Qw"
      },
      "source": [
        "PERCOBAAN 4: Embeddings + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kadufm3W2vb",
        "outputId": "cddaa04c-4344-4674-a75b-79111521079a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step\n",
            "Akurasi: 0.4603\n"
          ]
        }
      ],
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_LEN))\n",
        "model_lstm.add(SpatialDropout1D(0.2))\n",
        "model_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_lstm.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=0) # Training diam-diam\n",
        "\n",
        "y_pred_lstm = np.argmax(model_lstm.predict(X_test_pad), axis=1)\n",
        "acc_lstm = accuracy_score(y_test, y_pred_lstm)\n",
        "results.append({\"Model\": \"LSTM\", \"Feature\": \"Embeddings\", \"Accuracy\": acc_lstm})\n",
        "print(f\"Akurasi: {acc_lstm:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JeFSro5XFvF"
      },
      "source": [
        "PERCOBAAN 5: Embeddings + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJTInpwwXGy8",
        "outputId": "7e92c21f-f238-40bc-fd07-fd0735ab1f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "Akurasi: 0.3492\n"
          ]
        }
      ],
      "source": [
        "inputs = Input(shape=(MAX_LEN,))\n",
        "emb_layer = Embedding(MAX_WORDS, EMBEDDING_DIM)(inputs)\n",
        "\n",
        "# Transformer Block Sederhana\n",
        "att = MultiHeadAttention(num_heads=2, key_dim=EMBEDDING_DIM)(emb_layer, emb_layer)\n",
        "att = Dropout(0.1)(att)\n",
        "norm = LayerNormalization(epsilon=1e-6)(att + emb_layer)\n",
        "pool = GlobalAveragePooling1D()(norm)\n",
        "dropout = Dropout(0.1)(pool)\n",
        "outputs = Dense(num_classes, activation=\"softmax\")(dropout)\n",
        "\n",
        "model_trans = Model(inputs=inputs, outputs=outputs)\n",
        "model_trans.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_trans.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "y_pred_trans = np.argmax(model_trans.predict(X_test_pad), axis=1)\n",
        "acc_trans = accuracy_score(y_test, y_pred_trans)\n",
        "results.append({\"Model\": \"Transformer\", \"Feature\": \"Embeddings\", \"Accuracy\": acc_trans})\n",
        "print(f\"Akurasi: {acc_trans:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQCtshktXK1y",
        "outputId": "da057047-2109-474c-a13b-c6face04beec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== REKAP HASIL ===\n",
            "                  Model     Feature  Accuracy\n",
            "0           Naive Bayes         BoW  0.619048\n",
            "1           Naive Bayes         BoW  0.619048\n",
            "6           Naive Bayes         BoW  0.619048\n",
            "11          Naive Bayes         BoW  0.619048\n",
            "3                   SVM      TF-IDF  0.603175\n",
            "4                  LSTM  Embeddings  0.603175\n",
            "7   Logistic Regression      TF-IDF  0.603175\n",
            "2   Logistic Regression      TF-IDF  0.603175\n",
            "13                  SVM      TF-IDF  0.603175\n",
            "8                   SVM      TF-IDF  0.603175\n",
            "12  Logistic Regression      TF-IDF  0.603175\n",
            "9                  LSTM  Embeddings  0.476190\n",
            "5           Transformer  Embeddings  0.460317\n",
            "14                 LSTM  Embeddings  0.460317\n",
            "10          Transformer  Embeddings  0.396825\n",
            "15          Transformer  Embeddings  0.349206\n",
            "\n",
            "Detail untuk Model Terbaik (Classification Report):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.50      0.45      0.47        20\n",
            "      netral       0.52      0.64      0.57        22\n",
            "     positif       0.83      0.71      0.77        21\n",
            "\n",
            "    accuracy                           0.60        63\n",
            "   macro avg       0.62      0.60      0.60        63\n",
            "weighted avg       0.62      0.60      0.61        63\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== REKAP HASIL ===\")\n",
        "df_results = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)\n",
        "print(df_results)\n",
        "\n",
        "print(\"\\nDetail untuk Model Terbaik (Classification Report):\")\n",
        "# Kita ambil contoh SVM sebagai baseline report\n",
        "print(classification_report(y_test, y_pred_svm, target_names=le.classes_))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
